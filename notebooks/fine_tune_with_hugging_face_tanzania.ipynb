{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PisUoMdHCDNq"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Fine-tune MedGemma with Hugging Face\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedgemma%2Fmain%2Fnotebooks%2Ffine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on HuggingFace\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates fine-tuning MedGemma on an image and text dataset for a vision task using Hugging Face libraries.\n",
        "\n",
        "In this guide, you will use Hugging Face's [Transformer Reinforcement Learning (`TRL`)](https://github.com/huggingface/trl) library to train the model with Supervised Fine-Tuning (SFT), utilizing [Quantized Low-Rank Adaptation (QLoRA)](https://arxiv.org/abs/2305.14314) to reduce computational costs while maintaining high performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcamD7TMKjbt"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with sufficient resources to fine-tune the MedGemma model. **Note:** This guide requires a GPU that supports bfloat16 data type and has at least 40 GB of memory.\n",
        "\n",
        "You can run this notebook in Google Colab using an A100 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **A100 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2vUYdJKlCZ"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syRJ6pl4KpEL"
      },
      "source": [
        "### Configure your HF token\n",
        "\n",
        "Generate a Hugging Face `write` access token by going to [settings](https://huggingface.co/settings/tokens). **Note:** Make sure that the token has write access to push the fine-tuned model to Hugging Face Hub.\n",
        "\n",
        "If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the 🔑 Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8NaHvwRqL3-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
        "    # Use secret if running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "else:\n",
        "    # Store Hugging Face data under `/content` if running in Colab Enterprise\n",
        "    if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
        "        os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "    # Authenticate with Hugging Face\n",
        "    from huggingface_hub import get_token\n",
        "    if get_token() is None:\n",
        "        from huggingface_hub import notebook_login\n",
        "        notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFU3B09TKuQf"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sgv3DCPIA5p-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3861f2-93af-4fdf-eb33-2a4bff6e6a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade --quiet bitsandbytes datasets evaluate peft tensorboard transformers trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Tanzania dataset"
      ],
      "metadata": {
        "id": "0Pg34J3aWd36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m3VwUWawgKaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load this file /content/drive/MyDrive/AI4Thyroid/ProcessedData/train_dataset.csv into a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/AI4Thyroid/ProcessedData/train_dataset.csv')\n",
        "\n",
        "# You can now access and work with the dataframe using df_train\n",
        "# For example, to see the first few rows:\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "id": "ARNV_A-Oa91X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "def crop_background(img_path):\n",
        "  # Check if the image path is valid\n",
        "  if not os.path.exists(img_path):\n",
        "      print(f\"Warning: Image file not found at {img_path}. Skipping.\")\n",
        "      return None\n",
        "\n",
        "  try:\n",
        "      image = Image.open(img_path).convert('RGB')\n",
        "      original_img_np = np.array(image)\n",
        "\n",
        "      # Apply global thresholding to remove black background\n",
        "      # Convert to grayscale for thresholding\n",
        "      #gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "      #_, binary_mask = cv2.threshold(gray_image, self.threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "      # Convert to grayscale for thresholding\n",
        "      gray_img = cv2.cvtColor(original_img_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "      # Apply global thresholding to create a mask of non-black pixels\n",
        "      # Assuming black is close to 0. Adjust threshold if necessary.\n",
        "      _, thresh = cv2.threshold(gray_img, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "      # Find contours in the thresholded image\n",
        "      contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "\n",
        "      # Find the bounding box of the largest contour (likely the main object)\n",
        "      if contours:\n",
        "          # Find the contour with the maximum area\n",
        "          largest_contour = max(contours, key=cv2.contourArea)\n",
        "          x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "\n",
        "          # Crop the original image based on the bounding box\n",
        "          cropped_img_np = original_img_np[y:y+h, x:x+w]\n",
        "      else:\n",
        "          # If no significant contours are found, use the original image\n",
        "          cropped_img_np = original_img_np\n",
        "\n",
        "\n",
        "\n",
        "      # Apply the mask to the original image\n",
        "      # masked_image_np = cv2.bitwise_and(image_np, image_np, mask=binary_mask)\n",
        "\n",
        "      # Convert back to PIL Image\n",
        "      image = Image.fromarray(cropped_img_np)\n",
        "      return image\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading or processing image {img_path}: {e}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "9b0r10sQfycI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: test the crop_background function using a random image from df_train\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a random image path from the training dataframe\n",
        "random_index = random.randint(0, len(df_train) - 1)\n",
        "random_image_path = df_train.iloc[random_index]['image_paths']\n",
        "\n",
        "# Apply the crop_background function to the random image\n",
        "cropped_image = crop_background(random_image_path)\n",
        "\n",
        "# Display the original and cropped images (optional)\n",
        "if cropped_image:\n",
        "    original_image = Image.open(random_image_path).convert('RGB')\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cropped_image)\n",
        "    plt.title(\"Cropped Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Failed to crop image.\")"
      ],
      "metadata": {
        "id": "m9rEyEZ_esGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: user Datasets.from_generator to create a hugging face image dataset using using image_paths and Histopathology in df_train, make sure to crop the images first using the crop_background function above, show the progress along the way\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "def generate_dataset(df):\n",
        "    for index, row in df.iterrows():\n",
        "        image_path = row['image_paths']\n",
        "        histopathology = row['Histopathology']\n",
        "\n",
        "        cropped_image = crop_background(image_path)\n",
        "\n",
        "        if cropped_image is not None:\n",
        "            yield {\"image\": cropped_image, \"Histopathology\": histopathology}\n",
        "\n",
        "# Create the dataset using Datasets.from_generator with show_progress=True\n",
        "dataset = Dataset.from_generator(\n",
        "    lambda: generate_dataset(df_train),\n",
        "    features=None, # Automatically infer features\n",
        "    keep_in_memory=False, # Use streaming if the dataset is large\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "# You can now work with the 'dataset' object.\n",
        "# For example, to see the first example:\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "_-WFRArxbGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "oUUM73GduJK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: show one example from the dataset\n",
        "\n",
        "hf_dataset[0][\"image\"]"
      ],
      "metadata": {
        "id": "BxpxW96kesEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrnDCZvvzWtS"
      },
      "source": [
        "## Prepare fine-tuning dataset\n",
        "\n",
        "This notebook uses the [NCT-CRC-HE-100K](https://zenodo.org/records/1214456) dataset, containing image patches from histological images of human colorectal cancer (CRC) and normal tissue, to fine-tune MedGemma to classify tissue types.\n",
        "\n",
        "**Note:** The full NCT-CRC-HE-100K dataset contains 100K samples. By default this guide only uses a subset with 10,000 samples to keep the training example small, but you can adjust this number if you want to experiment.\n",
        "\n",
        "**Dataset citation:** Kather, J. N., Halama, N., & Marx, A. (2018). 100,000 histological images of human colorectal cancer and healthy tissue (v0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1214456"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q6p2msnQ1rE"
      },
      "source": [
        "Download the dataset. This step may take around 15 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr3Iy7rq85LF"
      },
      "outputs": [],
      "source": [
        "! wget -nc -q \"https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip\"\n",
        "! unzip -q NCT-CRC-HE-100K.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTYiM4RAjJAo"
      },
      "source": [
        "Load the data using the Hugging Face `datasets` library. Then, create train and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4T4OJtA-Tpy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_size = 9000  # @param {type: \"number\"}\n",
        "validation_size = 1000  # @param {type: \"number\"}\n",
        "\n",
        "data = load_dataset(\"./NCT-CRC-HE-100K\", split=\"train\")\n",
        "data = data.train_test_split(\n",
        "    train_size=train_size,\n",
        "    test_size=validation_size,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "# Use the test split as the validation set\n",
        "data[\"validation\"] = data.pop(\"test\")\n",
        "\n",
        "# Display dataset details\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V03H6cfnofY_"
      },
      "source": [
        "Inspect a sample data point, which contains:\n",
        "\n",
        "* `image`: image patch as a `PIL` image object\n",
        "* `label`: integer class label corresponding to tissue type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC4cn63B-O0u"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQmBG7Dx-WRs"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dlyBQdpAAD"
      },
      "source": [
        "For this classification task, create a multiple-choice question prompt and preprocess the data into a multimodal conversational format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry64yT0juQBD"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "TISSUE_CLASSES = [\n",
        "    \"A: adipose\",\n",
        "    \"B: background\",\n",
        "    \"C: debris\",\n",
        "    \"D: lymphocytes\",\n",
        "    \"E: mucus\",\n",
        "    \"F: smooth muscle\",\n",
        "    \"G: normal colon mucosa\",\n",
        "    \"H: cancer-associated stroma\",\n",
        "    \"I: colorectal adenocarcinoma epithelium\"\n",
        "]\n",
        "\n",
        "options = \"\\n\".join(TISSUE_CLASSES)\n",
        "PROMPT = f\"What is the most likely tissue type shown in the histopathology image?\\n{options}\"\n",
        "\n",
        "\n",
        "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": TISSUE_CLASSES[example[\"label\"]],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBf5lbmkJ59x"
      },
      "source": [
        "Apply the processing function on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFiPNp30_X6z"
      },
      "outputs": [],
      "source": [
        "data = data.map(format_data)\n",
        "\n",
        "# Display a processed data sample\n",
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOtCBFfSWyS"
      },
      "source": [
        "## Fine-tune the model with LoRA\n",
        "\n",
        "Traditional fine-tuning of large language models is resource-intensive because it requires adjusting billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is Low-Rank Adaptation (LoRA), which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices. In QLoRA, the base model is quantized to 4-bit before its weights are frozen, then LoRA adapter layers are attached and trained.\n",
        "\n",
        "This notebook demonstrates supervised fine-tuning MedGemma with QLoRA using the `SFTTrainer` from the Hugging Face `TRL` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXh0RtBk_Xc6"
      },
      "source": [
        "### Load model from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGkKFxVM5Fb"
      },
      "source": [
        "Initialize the quantization configuration and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-esHCwnQFye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "# Check if GPU supports bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Use right padding to avoid issues during training\n",
        "processor.tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQmPZGy_7NV"
      },
      "source": [
        "### Set up for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlPOkUjhH1P"
      },
      "source": [
        "Create a [`LoraConfig`](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig). It will be provided to the `SFTTrainer`, which supports built-in integration with the Hugging Face `PEFT` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCWNm3wtMBqX"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrDwrz6iD3Yl"
      },
      "source": [
        "Define a custom data collator that processes examples containing text and images and returns batches of data in the expected model input format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRPETOTGV81b"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "def collate_fn(examples: list[dict[str, Any]]):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        images.append([example[\"image\"].convert(\"RGB\")])\n",
        "        texts.append(processor.apply_chat_template(\n",
        "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "        ).strip())\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, with the padding and image tokens masked in\n",
        "    # the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens that are not used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsaon7JoBq5H"
      },
      "source": [
        "Configure training parameters in an [`SFTConfig`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6W_gQmfRXWx"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "num_train_epochs = 1  # @param {type: \"number\"}\n",
        "learning_rate = 2e-4  # @param {type: \"number\"}\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-sft-lora-crc100k\",            # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
        "    per_device_train_batch_size=4,                           # Batch size per device during training\n",
        "    per_device_eval_batch_size=4,                            # Batch size per device during evaluation\n",
        "    gradient_accumulation_steps=4,                           # Number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
        "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
        "    logging_steps=50,                                        # Number of steps between logs\n",
        "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
        "    eval_steps=50,                                           # Number of steps between evaluations\n",
        "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
        "    bf16=True,                                               # Use bfloat16 precision\n",
        "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
        "    push_to_hub=True,                                        # Push model to Hub\n",
        "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
        "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
        "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnPGToATAFCN"
      },
      "source": [
        "### Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rMdSj1Tj1K"
      },
      "source": [
        "Construct an [`SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) using the previously defined LoRA configuration, custom data collator, and training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWcynpm0MHDc"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoHlpSKKVbDb"
      },
      "source": [
        "Launch the fine-tuning process.\n",
        "\n",
        "**Note**: This may take around 3 hours to run using the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viHUr_jJfHCG"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBotQy2xAU6-"
      },
      "source": [
        "Save the final model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-WxJHtQZJII"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRXx3qwAdjd"
      },
      "source": [
        "Free up memory before proceeding to evaluate and test the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El9e2_8xZLLi"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf9QkPFAfegp"
      },
      "source": [
        "## Evaluate the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZCfth8Nn3O"
      },
      "source": [
        "### Prepare test dataset\n",
        "\n",
        "The [CRC-VAL-HE-7K](https://zenodo.org/records/1214456) dataset contains image patches from patients with colorectal adenocarcinoma and does not overlap with NCT-CRC-HE-100K. It can be used as the test dataset to evaluate the fine-tuned MedGemma model.\n",
        "\n",
        "**Note:** The full CRC-VAL-HE-7K dataset contains over 7K samples. By default this guide only uses a subset with 1,000 samples to keep the evaluation example small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eziDK33PdB5f"
      },
      "source": [
        "Download and prepare the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv82XT9fVI6y"
      },
      "outputs": [],
      "source": [
        "! wget -nc -q \"https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\"\n",
        "! unzip -q CRC-VAL-HE-7K.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECfWV0ToVZW2"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "\n",
        "test_data = load_dataset(\"./CRC-VAL-HE-7K\", split=\"train\")\n",
        "test_data = test_data.shuffle(seed=42).select(range(1000))\n",
        "test_data = test_data.map(format_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1Mfl6wWTHk"
      },
      "source": [
        "### Set up for evaluation\n",
        "\n",
        "Load the accuracy and F1 score metrics to evaluate the model's performance on the classfication task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdpSoOtLWkBX"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# Ground-truth labels\n",
        "REFERENCES = test_data[\"label\"]\n",
        "\n",
        "\n",
        "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
        "    metrics = {}\n",
        "    metrics.update(accuracy_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "    ))\n",
        "    metrics.update(f1_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "        average=\"weighted\",\n",
        "    ))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdl6yTAzczn7"
      },
      "source": [
        "Define a postprocessing function to convert responses to integer class labels before computing metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5MEos4Bcy9L"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "# Rename the class names to the tissue classes, `X: tissue type`\n",
        "test_data = test_data.cast_column(\n",
        "    \"label\",\n",
        "    ClassLabel(names=TISSUE_CLASSES)\n",
        ")\n",
        "\n",
        "LABEL_FEATURE = test_data.features[\"label\"]\n",
        "# Mapping to alternative label format, `(X) tissue type`\n",
        "ALT_LABELS = dict([\n",
        "    (label, f\"({label.replace(': ', ') ')}\") for label in TISSUE_CLASSES\n",
        "])\n",
        "\n",
        "\n",
        "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
        "    response_text = prediction[0][\"generated_text\"]\n",
        "    if do_full_match:\n",
        "        return LABEL_FEATURE.str2int(response_text)\n",
        "    for label in TISSUE_CLASSES:\n",
        "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
        "        if label in response_text or ALT_LABELS[label] in response_text:\n",
        "            return LABEL_FEATURE.str2int(label)\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh8t3-oUV9X7"
      },
      "source": [
        "### Compute baseline metrics on the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj-8jpcPcPUm"
      },
      "source": [
        "Load the pretrained model using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ged1Z8Tjb918"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pt_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "pt_pipe.model.generation_config.do_sample = False\n",
        "pt_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96XmD5loi6aX"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cnCCpuSi6aX"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=40,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "pt_predictions = [postprocess(out) for out in pt_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5slBRejjIOe"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Rhs-0LfMPE"
      },
      "outputs": [],
      "source": [
        "pt_metrics = compute_metrics(pt_predictions)\n",
        "print(f\"Baseline metrics: {pt_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRuNOOAkWFCD"
      },
      "source": [
        "### Compute metrics on the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwzxpFhsA0Yi"
      },
      "source": [
        "Load the base model with the fine-tuned LoRA adapter using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsy5waU6jG7W"
      },
      "outputs": [],
      "source": [
        "ft_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=args.output_dir,\n",
        "    processor=processor,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "ft_pipe.model.generation_config.do_sample = False\n",
        "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
        "# Use left padding during inference\n",
        "processor.tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjw54wGjnWwT"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjP2BFDcjJxN"
      },
      "outputs": [],
      "source": [
        "ft_outputs = ft_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=20,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_gxaSxRqWjG"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YmLRYMRfpon"
      },
      "outputs": [],
      "source": [
        "ft_metrics = compute_metrics(ft_predictions)\n",
        "print(f\"Fine-tuned metrics: {ft_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Smu13_YlWG"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fine_tune_with_hugging_face.ipynb",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}